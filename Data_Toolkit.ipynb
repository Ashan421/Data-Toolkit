{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Data Toolkit\n",
        "\n",
        "1. What is NumPy, and why is it widely used in Python?\n",
        "\n",
        " - NumPy, short for Numerical Python, is a fundamental open-source library for scientific computing in Python. It provides powerful tools for working with large, multi-dimensional arrays and matrices, along with a vast collection of high-level mathematical functions to operate on these arrays.\n",
        " - NumPy is the backbone of numerical and scientific computing in Python. Its efficient array object, high-performance operations, extensive mathematical functions, and seamless integration with other libraries make it an indispensable tool for data scientists, engineers, researchers, and anyone working with numerical data in Python.\n",
        "\n",
        "   1. Efficient Multi-dimensional Arrays:\n",
        "    - NumPy's core is the ndarray object, which is an N-dimensional array. Unlike Python lists, NumPy arrays are designed for numerical operations and are homogeneously typed, meaning all elements within an array must be of the same data type. This allows for more efficient memory storage and faster computations.\n",
        "\n",
        "    - They are implemented in C and Fortran, making operations on them significantly faster than equivalent operations on standard Python lists, especially for large datasets.\n",
        "\n",
        "   2. Performance and Speed:\n",
        "    - NumPy achieves its speed through vectorization and broadcasting. Instead of writing explicit loops in Python, NumPy allows us to perform operations on entire arrays at once. These operations are executed by highly optimized, pre-compiled C code. This significantly reduces computation time, making it ideal for large-scale numerical tasks.\n",
        "\n",
        "    - This \"vectorized\" code is more concise, easier to read, and typically has fewer bugs.\n",
        "\n",
        "   3. Rich Set of Mathematical Functions:\n",
        "    - NumPy offers a comprehensive collection of mathematical functions for linear algebra, Fourier transforms, random number generation, statistical operations, and more. This makes it a one-stop shop for many scientific and data analysis tasks.\n",
        "\n",
        "    - It supports element-wise operations, comparisons, and applying universal functions to entire arrays.\n",
        "\n",
        "   4. Foundation for Other Libraries:\n",
        " - Many other popular scientific and data science libraries in the Python ecosystem are built on top of NumPy arrays or extensively use them as their primary data structure. Examples include:\n",
        "\n",
        "    - Pandas: Uses NumPy arrays internally for its DataFrames.\n",
        "    - SciPy: Builds on NumPy to provide more advanced scientific computing capabilities.\n",
        "\n",
        "    - Matplotlib: Uses NumPy arrays for plotting and visualization.\n",
        "\n",
        "    - Scikit-learn: A machine learning library that heavily relies on NumPy arrays for data representation and model training.\n",
        "\n",
        "    - TensorFlow and PyTorch: Deep learning frameworks that use NumPy arrays for their underlying computations.\n",
        "\n",
        "   5. Simplified Code and Readability:\n",
        "    - The vectorized nature of NumPy code often makes it more concise and readable, resembling standard mathematical notation. This simplifies the development process and makes it easier to understand and maintain numerical algorithms.\n",
        "\n",
        "   6. Memory Efficiency:\n",
        "    - NumPy arrays use less memory than Python lists for storing numerical data because they store elements of a fixed size and type contiguously in memory. This is crucial when dealing with very large datasets.\n",
        "\n",
        "2. How does broadcasting work in NumPy?\n",
        "\n",
        " - Broadcasting in NumPy refers to the ability to perform arithmetic operations on arrays of different shapes. This feature allows NumPy to treat arrays with different dimensions during arithmetic operations, ensuring they have compatible shapes without making unnecessary copies of data.Broadcasting works by stretching the smaller array across the larger array so that they have compatible shapes. This process is efficient because it avoids making needless copies of data and allows operations to be vectorized, which means they are executed in C rather than Python.\n",
        "\n",
        "   1.  Dimension Alignment: NumPy compares the shapes of the two arrays starting from the trailing dimension and moving leftward.\n",
        "\n",
        "   2. Compatibility Check for Each Dimension: For each dimension being compared, two dimensions are considered compatible if:\n",
        "    - They are equal in size, OR\n",
        "    - One of them has a size of 1.\n",
        "\n",
        "   3. Dimension Expansion:\n",
        "\n",
        "    - If a dimension has a size of 1 in one array and a larger size in the other, the array with the dimension of size 1 is \"stretched\" along that dimension to match the larger size.\n",
        "    - If one array has fewer dimensions than the other, its shape is left-padded with ones until both shapes have the same length. For example, a 1D array of shape (3,) when compared with a 2D array of shape (2, 3) would be treated as (1, 3).\n",
        "\n",
        "   4. Incompatibility: If at any point during the comparison, two corresponding dimensions are not equal and neither of them is 1, then the arrays are not broadcastable, and NumPy will raise a ValueError.\n",
        "\n",
        "   5. Resulting Shape: The shape of the output array will be the maximum size along each dimension of the input arrays after applying the broadcasting rules.\n",
        "\n",
        "          import numpy as np\n",
        "\n",
        "          A = np.array([[10, 20, 30],\n",
        "          [40, 50, 60]])\n",
        "  \n",
        "          B = np.array([1, 2, 3])\n",
        "\n",
        "          result = A + B\n",
        "          print(result)\n",
        "\n",
        "3. What is a Pandas DataFrame?\n",
        "\n",
        " - A Pandas DataFrame is one of the core data structures in the Pandas library, which is widely used for data manipulation and analysis in Python.The most fundamental and widely used data structures in the Pandas library, built on top of NumPy arrays, similar to an Excel spreadsheet or a SQL table, but much more powerful and flexible.\n",
        "\n",
        "    - 2-Dimensional: It's a two-dimensional labeled data structure with rows and columns.\n",
        "    - Labeled Axes: Both rows and columns have labels.\n",
        "    - Heterogeneous Data: Each column can hold different types of data.\n",
        "    - Size Mutable: We can add or remove columns and rows easily.\n",
        "    - Built-in Methods: Comes with a rich set of methods for filtering, grouping, reshaping, aggregating, and visualizing data.\n",
        "\n",
        "           import pandas as pd\n",
        "\n",
        "           data = {'Name': ['Ashan', 'Bob', 'Charlie'],\n",
        "                   'Age': [21, 30, 35],\n",
        "                   'City': ['Patna', 'Paris', 'London']}\n",
        "  \n",
        "           df = pd.DataFrame(data)\n",
        "           print(df)\n",
        "\n",
        "4. Explain the use of the groupby() method in Pandas?\n",
        "\n",
        " - The groupby() method in Pandas is one of the most powerful and frequently used functions for data analysis. It allows us to split a DataFrame into groups based on some criteria, apply a function to each group independently, and then combine the results into a new DataFrame or Series. This process is often referred to as the \"split-apply-combine\" strategy.\n",
        "     - Split: The groupby() method divides the DataFrame into multiple sub-DataFrames, where each sub-DataFrame corresponds to a unique combination of values in the column(s) we're grouping by.\n",
        "     - Apply: A function is then applied to each individual group.\n",
        "     - Combine: The results of these individual operations are then combined back together into a single, cohesive data structure.\n",
        "\n",
        "           import pandas as pd\n",
        "\n",
        "           # Sample data\n",
        "           data = {\n",
        "           'Department': ['Sales', 'Sales', 'HR', 'HR', 'IT'],\n",
        "           'Employee': ['Ashan', 'Bob', 'Coco', 'David', 'Eleven'],\n",
        "           'Salary': [50000, 60000, 45000, 47000, 70000]\n",
        "            }\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Group by Department and calculate average salary\n",
        "            grouped = df.groupby('Department')['Salary'].mean()\n",
        "            print(grouped)\n",
        "\n",
        "   1. Aggregation : Aggregation functions compute a single summary statistic for each group. This is the most common use of groupby().\n",
        "\n",
        "     Examples of aggregation functions:\n",
        "    - sum(): Calculates the sum of values in each group.\n",
        "    - mean(): Calculates the average of values in each group.\n",
        "    - count(): Counts non-null values in each group.\n",
        "    - min(), max(): Finds the minimum/maximum value in each group.\n",
        "    - std(), var(): Calculates standard deviation/variance.\n",
        "    - size(): Counts the number of rows in each group.\n",
        "    - first(), last(): Returns the first/last value in each group.\n",
        "\n",
        "   2.  Transformation : Transformation operations return a Series or DataFrame with the same index as the original object, ensuring the result has the same shape as the original data. They are useful for normalizing data within groups, filling missing values within groups, etc.\n",
        "   3. Filtration : Filtration operations allow us to discard entire groups based on a condition applied to the group's data.\n",
        "   4. . Applying Custom Functions (.apply()) : For more complex, custom operations that don't fit into aggregation, transformation, or filtration, we can use the apply() method. This allows us to pass any arbitrary function to each group.\n",
        "\n",
        "\n",
        "5. Why is Seaborn preferred for statistical visualizations?\n",
        "\n",
        "\n",
        " - Seaborn is a Python data visualization library that is built on top of Matplotlib and integrates tightly with Pandas data structures. While Matplotlib provides the fundamental building blocks for creating plots, Seaborn specializes in creating aesthetically pleasing and statistically informative graphics with less code.\n",
        "\n",
        "  1. High-Level Interface for Statistical Graphics:\n",
        "   - Seaborn provides a higher-level API compared to Matplotlib. This means we can create complex statistical plots with fewer lines of code. Instead of manually handling axes, legends, and aesthetic details, Seaborn functions often intelligently figure out what we want to plot based on the type of data we provide.\n",
        "   - It abstracts away much of the boilerplate code required by Matplotlib, making plotting quicker and more intuitive, especially for common statistical tasks.\n",
        "\n",
        "  2. Focus on Statistical Plotting:\n",
        "   - Seaborn is specifically designed with statistical analysis in mind. It offers a wide array of specialized plots that are crucial for understanding data distributions, relationships between variables, and patterns within groups.\n",
        "   - Examples include:\n",
        "      - Distribution Plots: histplot, kdeplot, ecdfplot for visualizing univariate or bivariate distributions.\n",
        "      - Relational Plots: scatterplot, lineplot for showing relationships between two or more variables, often with statistical estimates. The relplot function acts as a \"figure-level\" interface for these.\n",
        "      - Categorical Plots: boxplot, violinplot, swarmplot, barplot, countplot for visualizing relationships between numerical and categorical variables. The catplot function acts as a \"figure-level\" interface for these.\n",
        "      - Regression Plots: lmplot, regplot for easily visualizing linear regression models and their confidence intervals.\n",
        "      - Matrix Plots: heatmap for visualizing correlation matrices or other matrix-like data, and clustermap for hierarchical clustering.\n",
        "      - Multi-plot Grids: FacetGrid, PairGrid, and functions like relplot, catplot, displot automatically create grids of subplots based on categorical variables, allowing for easy comparison across different subsets of data.\n",
        "\n",
        "  3. Aesthetics and Professional-Looking Defaults:\n",
        "  - Seaborn comes with attractive default themes and color palettes that make our plots look professional and visually appealing right out of the box. We don't need extensive customization to get good-looking results.\n",
        "  - It includes pre-set styles like 'darkgrid', 'whitegrid', 'dark', 'white', and 'ticks' that can be applied with a single line of code (sns.set_style()).\n",
        "  - The default color palettes are often perceptually uniform and colorblind-friendly.\n",
        "\n",
        "  4. Seamless Integration with Pandas DataFrames:\n",
        "  - Seaborn is designed to work directly with Pandas DataFrames. We can typically pass a DataFrame to a Seaborn plotting function and specify column names for our x, y, hue, size, etc., arguments.\n",
        "   - This deep integration simplifies the data visualization workflow, as we often don't need to extract specific Series or arrays from our DataFrame before plotting. Seaborn handles the mapping and aggregation internally.\n",
        "\n",
        "  5. Seamless Integration with Pandas DataFrames:\n",
        "   - Seaborn is designed to work directly with Pandas DataFrames. We can typically pass a DataFrame to a Seaborn plotting function and specify column names for our x, y, hue, size, etc., arguments.\n",
        "   - This deep integration simplifies the data visualization workflow, as we often don't need to extract specific Series or arrays from our DataFrame before plotting. Seaborn handles the mapping and aggregation internally.\n",
        "\n",
        "  6. Faceting for Complex Data Exploration:\n",
        "   - Seaborn's \"figure-level\" functions allow us to easily create complex grid plots where we can split a visualization across rows, columns, or even different plot types based on categorical variables. This is incredibly powerful for exploring multi-dimensional datasets.\n",
        "\n",
        "6. What are the differences between NumPy arrays and Python lists ?\n",
        "\n",
        "   - NumPy arrays and Python lists are used to store collections of data, they have fundamental differences that make them suitable for different tasks.\n",
        "   1. Data Type\n",
        "   - NumPy Arrays: Homogeneous, meaning all elements must be of the same data type.\n",
        "   - Python Lists: Heterogeneous, allowing elements of different data types.\n",
        "   2. Performance\n",
        "   - NumPy Arrays: Faster for numerical computations due to optimized C-based implementation and contiguous memory storage.\n",
        "   - Python Lists: Slower for numerical operations as they store references to objects, not raw data.\n",
        "   3. Memory Efficiency\n",
        "   - NumPy Arrays: More memory-efficient because they store data in a compact, contiguous block.\n",
        "   - Python Lists: Less efficient as they store pointers to objects, requiring additional memory.\n",
        "   4. Functionality\n",
        "   - NumPy Arrays: Provide a wide range of mathematical, statistical, and linear algebra operations directly.\n",
        "   - Python Lists: Lack built-in numerical operations; require manual implementation or external libraries.\n",
        "   5. Flexibility\n",
        "   - NumPy Arrays: Less flexible for mixed data types or dynamic resizing.\n",
        "   - Python Lists: Highly flexible, allowing mixed data types and dynamic resizing.\n",
        "   - Example:\n",
        "\n",
        "          import numpy as np\n",
        "\n",
        "         # NumPy Array\n",
        "         arr = np.array([1, 2, 3])\n",
        "         print(arr * 2)  # Efficient element-wise multiplication\n",
        "\n",
        "         # Python List\n",
        "         lst = [1, 2, 3]\n",
        "         print([x * 2 for x in lst])  # Requires a loop for similar operation\n",
        "\n",
        "\n",
        "7.  What is a heatmap, and when should it be used?\n",
        "\n",
        " - A heatmap is a graphical representation of data where the individual values contained in a matrix are represented by colors. The variation in color allows for a quick and intuitive understanding of patterns, density, and magnitudes across two dimensions.It's a fantastic way to visualize patterns, correlations, and intensity in data at a glance.\n",
        "\n",
        "     - Visualize Relationships/Correlations between Many Variables.\n",
        "     - Identify Patterns and Trends in Tabular Data.\n",
        "     - Explore Time-Series Data with Seasonal Patterns.\n",
        "     - Compare Categorical Data Across Two Dimensions.\n",
        "     - Visualize Density or Intensity on a Surface.\n",
        "     - Analyze User Behavior on Websites.\n",
        "\n",
        "  - Advantages of Heatmaps:\n",
        "     - Quick Overview: Provides an immediate visual summary of large datasets.\n",
        "     - Pattern Detection: Excellent for identifying trends, clusters, and outliers.\n",
        "     - Intuitive: Color encoding is easy to understand for most people.\n",
        "     - Compact: Can display a lot of information in a relatively small space.\n",
        "\n",
        "8. What does the term “vectorized operation” mean in NumPy?\n",
        "\n",
        " - Vectorization in NumPy is a method of performing operations on entire arrays without explicit loops.\n",
        "\n",
        " - Standard Python for loops are relatively slow for large-scale numerical computations. This is because:\n",
        "\n",
        "     - Dynamic Typing: Python variables are dynamically typed, meaning the type of an object can change during runtime. In a loop, Python has to check the type of each element at every iteration, which adds significant overhead.\n",
        "     - Overhead of Python Interpreter: Each iteration of a Python loop involves calls to the Python interpreter, which has its own overhead.\n",
        "     - Non-Contiguous Memory: Python lists can store heterogeneous data types, and their elements are not necessarily stored contiguously in memory. This can lead to slower memory access.\n",
        "\n",
        " - Examples of Vectorized Operations:\n",
        "    - Arithmetic Operations: +, -, *, /, **, %\n",
        "    - Comparison Operations: >, <, ==, !=, >=, <=\n",
        "    - Aggregation Functions: sum(), mean(), max(), min(), std().\n",
        "    - Functions: np.exp(), np.log(), np.sin(), etc.\n",
        "    - Logical: np.logical_and(), np.logical_or().\n",
        "\n",
        "9. How does Matplotlib differ from Plotly?\n",
        "\n",
        " - Matplotlib and Plotly are both powerful Python libraries for data visualization, but they cater to different needs and offer distinct advantages. The key differences lie in their level of interactivity, complexity of API, output formats, and ideal use cases.\n",
        "  \n",
        "  1. Interactivity\n",
        "   - Matplotlib:\n",
        "\n",
        "      - Primarily designed for static, publication-quality plots. When we create a plot with Matplotlib, it's typically rendered as a static image .\n",
        "      - While it has some interactive features, interactivity is not its core strength and often requires more setup.\n",
        "      - It's generally the go-to for figures in research papers, reports, or presentations where interactivity isn't a primary requirement.\n",
        "\n",
        "   - Plotly:\n",
        "\n",
        "      - Built from the ground up for interactive and dynamic visualizations. All Plotly plots are interactive by default, allowing users to zoom, pan, hover over data points for details, toggle trace visibility in the legend, and more, directly within a web browser or Jupyter Notebook.\n",
        "      - It's ideal for dashboards, web applications, and presentations where user exploration and interaction with the data are crucial.\n",
        "      - Plotly.js handles the rendering in the browser, providing a rich interactive experience.\n",
        "\n",
        "  2. Ease of Use & API Level\n",
        "   - Matplotlib:\n",
        "\n",
        "      - Offers a low-level, highly customizable API. This means we have fine-grained control over virtually every element of our plot.\n",
        "      - While this offers immense flexibility, it often requires more lines of code and can have a steeper learning curve for beginners to achieve sophisticated visualizations or custom layouts.\n",
        "      - It has both a procedural and an object-oriented interface.\n",
        "\n",
        "   - Plotly:\n",
        "\n",
        "      - Offers a higher-level API, especially with its plotly.express module, which allows US to create complex, aesthetically pleasing, and interactive plots with very few lines of code. It intelligently infers plot types and mappings from our DataFrame columns.\n",
        "      - For more intricate customizations, we can dive into plotly.graph_objects, which provides more granular control but is still generally more intuitive for interactivity than Matplotlib's low-level API.\n",
        "\n",
        "  3. Output and Embedding\n",
        "   - Matplotlib:\n",
        "\n",
        "       - Outputs primarily to static image files.\n",
        "       - Can be embedded into desktop GUI applications.\n",
        "       - Renders within Jupyter Notebooks as static images.\n",
        "\n",
        "   - Plotly:\n",
        "\n",
        "       - Outputs HTML files or JSON objects that are rendered by Plotly.js in a web browser.\n",
        "       - Plots can be easily embedded in web pages, dashboards, and interactive notebooks.\n",
        "       - Supports exporting to static image formats as well, but its main strength is its web-based nature.\n",
        "\n",
        "  4. Aesthetics and Default Styles\n",
        "  - Matplotlib:\n",
        "\n",
        "       - Default plots can sometimes appear basic or less aesthetically pleasing, often requiring significant customization to achieve a polished look.\n",
        "       - ibraries like Seaborn are built on Matplotlib to improve its aesthetics and provide high-level statistical plots.\n",
        "\n",
        "   - Plotly:\n",
        "\n",
        "       -  Offers attractive and modern default themes and color palettes that often look good out of the box, reducing the need for extensive styling.\n",
        "\n",
        "       - Its interactive nature inherently makes plots feel more dynamic and engaging.\n",
        "\n",
        "  5. Use Cases\n",
        "   - Matplotlib is ideal for:\n",
        "\n",
        "      - Static, high-quality plots for publications.\n",
        "      - When WE need ultimate control over every detail of the plot.\n",
        "      - As a foundational library for other plotting tools.\n",
        "      - Creating custom, complex, and specialized visualizations that might not be readily available in higher-level libraries.\n",
        "\n",
        "  - Plotly is ideal for:\n",
        "\n",
        "      - Interactive data exploration in Jupyter Notebooks.\n",
        "      - Creating web-based dashboards and analytical applications.\n",
        "      - Presentations where audience interaction with the data is desired.\n",
        "      - Rapid prototyping and quickly generating visually appealing, interactive charts with minimal code.\n",
        "      - Complex charts like 3D plots, geographic maps, and financial charts that are often more challenging to implement interactively in Matplotlib.\n",
        "\n",
        "10. What is the significance of hierarchical indexing in Pandas/\n",
        "\n",
        " - Hierarchical indexing, also known as MultiIndex, is a powerful and crucial feature in Pandas that allows you to have multiple levels of indexing on a single axis. Essentially, it enables you to work with and represent higher-dimensional data within the familiar 1D Series and 2D DataFrame structures.\n",
        "\n",
        "    1. Representing Higher-Dimensional Data in 2D:\n",
        "      - Pandas DataFrames are inherently 2D. However, real-world data often has more than two dimensions. Hierarchical indexing provides a way to \"flatten\" these higher dimensions into the rows or columns of a DataFrame while preserving the logical structure and relationships between the data points.\n",
        "      - Instead of creating multiple separate DataFrames or complex nested Python structures, you can consolidate all related information into a single, well-organized DataFrame.\n",
        "\n",
        "    2. Organized and Intuitive Data Structure:\n",
        "      - A MultiIndex makes your data more structured and easier to understand. When printed, it visually groups related rows , making it clear how different levels of categories relate to each other.\n",
        "      - This organized view helps in grasping the inherent hierarchy of your dataset at a glance.\n",
        "\n",
        "    3. Powerful and Flexible Data Selection:\n",
        "      - This is one of the most significant advantages. With a MultiIndex, you can easily select subsets of your data using \"partial indexing\" or \"partial slices.\" You can select data based on values from any level of the hierarchy, making complex queries straightforward.\n",
        "      - Using .loc [] with tuples or pd.IndexSlice, you can efficiently filter data across multiple levels of your index.\n",
        "      - Example: Select all sales for a particular State across all Cities, or all sales for a specific City within a given Region.\n",
        "\n",
        "    4. Enhanced Grouping and Aggregation:\n",
        "      - While groupby() can create a MultiIndex as an output when grouping by multiple columns, you can also directly groupby specific levels of an existing MultiIndex using the level argument. This streamlines aggregation tasks.\n",
        "      - Example: Calculate the average sales per State, then within each state, the average sales per City.\n",
        "\n",
        "    5. Reshaping Data:\n",
        "      - Hierarchical indexing is fundamental to reshaping operations like stack() and unstack().\n",
        "      - unstack(): Pivots inner index levels into columns, transforming a Series with a MultiIndex into a DataFrame, or a DataFrame with a MultiIndex into a wider DataFrame. This effectively moves a dimension from the index to the columns.\n",
        "      - stack(): Does the reverse, pivoting column labels into the innermost index level, effectively moving a dimension from columns to the index.\n",
        "      - These operations are incredibly useful for transforming data between \"long\" and \"wide\" formats, which is common in data preparation for analysis or visualization.\n",
        "\n",
        "    6. Data Alignment:\n",
        "      - Just like single-level indexes, MultiIndexes ensure automatic data alignment during operations like merging, joining, or arithmetic operations between DataFrames. Pandas will correctly align data based on all levels of the MultiIndex, preventing mismatches and ensuring data integrity.\n",
        "\n",
        "    7. Performance:\n",
        "      - While creating a MultiIndex has some overhead, performing operations  on a sorted MultiIndex can be very performant, as Pandas can leverage optimized algorithms. It's often recommended to call .sort_index after setting a MultiIndex, especially before performing complex selections.\n",
        "\n",
        "11.  What is the role of Seaborn’s pairplot() function ?\n",
        "\n",
        " - Seaborn's pairplot() function is a powerful tool for exploratory data analysis, especially when dealing with datasets that have multiple numerical variables. Its primary role is to create a grid of plots that visualizes the pairwise relationships between all numerical variables in a DataFrame, along with the univariate distribution of each individual variable.\n",
        "\n",
        "  1. Visualize Pairwise Relationships:\n",
        "\n",
        "   - For every possible combination of two numerical variables in our dataset, pairplot() generates a scatterplot.\n",
        "\n",
        "      - Correlations: Are two variables linearly related (positive or negative)?\n",
        "      - Patterns: Do they show non-linear relationships, clusters, or trends?\n",
        "      - Outliers: Are there any data points that deviate significantly from the general pattern?\n",
        "\n",
        "   - By looking at all these scatterplots simultaneously, we get a comprehensive overview of how each variable interacts with every other variable.\n",
        "\n",
        "  2. Visualize Univariate Distributions:\n",
        "\n",
        "   - Along the diagonal of the grid, pairplot() shows the univariate distribution of each individual numerical variable. By default, it uses a histogram or a Kernel Density Estimate (KDE) plot.\n",
        "\n",
        "   - This helps us understand the shape, spread, and central tendency of each variable on its own. Are they normally distributed? Skewed? Do they have multiple peaks?\n",
        "\n",
        "  3. Reveal Group-wise Patterns (with hue):\n",
        "\n",
        "   - One of the most powerful features of pairplot() is the hue parameter. By specifying a categorical column for hue, pairplot() will color-code all the plots based on the unique values of that categorical variable.\n",
        "   - This allows us to easily identify if the relationships or distributions between numerical variables differ across different groups. This is incredibly valuable for tasks like:\n",
        "      - Classification problems: Seeing if different classes are linearly separable or if their feature distributions overlap.\n",
        "      - Comparing subgroups: Understanding how different segments of our data behave (e.g., sales patterns for different customer segments, or health metrics for different treatment groups).\n",
        "\n",
        "  4. Early Detection of Data Problems/Insights:\n",
        "\n",
        "  - pairplot() provides a quick \"bird's-eye view\" of our entire dataset's numerical features. This is crucial in the early stages of EDA to:\n",
        "      - Spot potential issues.\n",
        "      - Formulate hypotheses about the data.\n",
        "      - Identify features that might be strongly correlated, which could be important for feature selection in machine learning.\n",
        "      - Determine if a simple linear model might be appropriate or if more complex relationships are at play.\n",
        "\n",
        "  5. High-Level Interface for PairGrid:\n",
        "\n",
        "   - Internally, pairplot() uses Seaborn's PairGrid to create the grid of plots. pairplot() provides a simplified, higher-level interface to PairGrid, making it easy to generate these common diagnostic plots with minimal code. For more advanced customization, we can directly use PairGrid.\n",
        "\n",
        "12. What is the purpose of the describe() function in Pandas?\n",
        "\n",
        " - The describe() function in Pandas is a vital method for exploratory data analysis. Its primary purpose is to generate descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset's distribution.\n",
        "\n",
        "  - For Numerical Columns : When describe() is called on a DataFrame or Series that contains numerical data, it returns the following statistics for each numerical column.\n",
        "\n",
        "     - count: The number of non-null (non-missing) values. This helps identify columns with missing data.\n",
        "     - mean: The average value of the column.\n",
        "     - std: The standard deviation, which measures the spread or dispersion of the data around the mean. A higher standard deviation indicates greater variability.\n",
        "     - min: The minimum value in the column.\n",
        "     - 25% (1st Quartile): The 25th percentile, meaning 25% of the data falls below this value. Also known as Q1.\n",
        "     - 50% (Median / 2nd Quartile): The 50th percentile, which is the middle value of the dataset when sorted. Also known as Q2.\n",
        "     - 75% (3rd Quartile): The 75th percentile, meaning 75% of the data falls below this value. Also known as Q3.\n",
        "     - max: The maximum value in the column.\n",
        "\n",
        " - For Non-Numerical Columns : We use describe or explicitly include non-numeric types, describe() will provide different statistics for those columns.\n",
        "\n",
        "     - count: Number of non-null values.\n",
        "     - unique: The number of unique values in the column.\n",
        "     - top: The most frequent value (mode).\n",
        "     - freq: The frequency (count) of the top value.\n",
        "\n",
        "13. Why is handling missing data important in Pandas?\n",
        "\n",
        " - Handling missing data in Pandas is absolutely crucial for several reasons, as neglecting it can lead to inaccurate or misleading insights, flawed models, and unreliable conclusions or even runtime errors in our data workflows.\n",
        "\n",
        " 1. Impact on Statistical Analysis:\n",
        "\n",
        "   - Biased Estimates: Many statistical calculations are sensitive to missing values. If data is missing in a non-random way , simply ignoring these missing values can lead to biased estimates and misrepresent the true population.you\n",
        "\n",
        "   - Reduced Statistical Power: Missing data reduces the effective sample size. A smaller sample size means less information, which can decrease the statistical power of our analysis, making it harder to detect true effects or relationships even if they exist.\n",
        "\n",
        "   - Increased Uncertainty: Missing data introduces more variability and uncertainty into our estimates, leading to wider confidence intervals and less precise conclusions.\n",
        "\n",
        "  2. Problems with Machine Learning Models:\n",
        "\n",
        "   - Most Algorithms Cannot Handle Missing Data: The vast majority of machine learning algorithms are designed to work with complete datasets. If you feed them data with NaN values, they will either:\n",
        "\n",
        "      - Throw an error: Preventing our model from training or running.\n",
        "\n",
        "      - Produce incorrect results: If they have a default way of handling NaNs that isn't appropriate for our data.\n",
        "\n",
        "   - Degraded Model Performance: Even if an algorithm can technically run, missing data can severely degrade its performance, leading to less accurate predictions, misclassifications, and overall unreliable models.\n",
        "\n",
        "   - Bias in Predictions: Similar to statistical analysis, if missingness is related to the outcome or other features, an unhandled missing data pattern can introduce bias into our model's predictions.\n",
        "\n",
        "  3. Data Integrity and Quality:\n",
        "\n",
        "   - Incomplete Picture: Missing values mean us have an incomplete view of our data. This makes it challenging to understand the full context or underlying patterns.\n",
        "   - Inconsistent Analysis: If missing values are treated inconsistently (e.g., dropping rows for one analysis but imputing for another), it can lead to inconsistent or non-comparable results.\n",
        "   - Trustworthiness: Data that isn't properly cleaned and handled for missingness is less trustworthy, undermining confidence in any insights derived from it.\n",
        "\n",
        "  4. Operational and Business Impact:\n",
        "\n",
        "   - Poor Decision-Making: If analysis results are flawed due to unhandled missing data, any business decisions based on those results can be incorrect or suboptimal, leading to financial losses, inefficient resource allocation, or missed opportunities.\n",
        "   - Operational Failures: In systems that rely on complete data, missing values can cause operational failures or produce nonsensical outputs.\n",
        "\n",
        "14.  What are the benefits of using Plotly for data visualization?\n",
        "\n",
        " - Plotly shines in data visualization because it blends interactivity, aesthetics, and flexibility - making it especially useful for dynamic data exploration and web-based applications.\n",
        "\n",
        "  1. Interactive Charts Out of the Box\n",
        "      - Zoom, pan, hover tooltips, clickable legends—all built-in.\n",
        "      - No extra configuration required to make charts engaging.\n",
        "\n",
        "  2. Web-Ready Visuals\n",
        "      - Produces HTML-friendly graphs that work seamlessly in:\n",
        "      - Jupyter notebooks\n",
        "      - Dash apps\n",
        "      - Static and dynamic websites\n",
        "\n",
        "  3. Beautiful and Customizable Styles\n",
        "      - Polished visual defaults—great for presentations.\n",
        "      - Easily tweak layout, colors, annotations, and templates.\n",
        "\n",
        "  4. Supports a Wide Variety of Plot Types\n",
        "      - Line, bar, pie, scatter, heatmap, histogram, box, 3D plots—and more.\n",
        "      -  Advanced options like geographical maps, ternary plots, sunburst, and waterfall charts.\n",
        "\n",
        "  5. Cross-Language Support\n",
        "      - Plotly isn't just for Python—it supports R, Julia, JavaScript, and more.\n",
        "\n",
        "  6. Real-Time Data Integration\n",
        "      - Perfect for dashboards and live monitoring tools.\n",
        "      - Easy to combine with frameworks like Dash for interactive web apps.\n",
        "\n",
        "  7. Responsive Layouts\n",
        "      - Automatically adjusts to different screen sizes—perfect for mobile and desktop user.\n",
        "\n",
        "15. How does NumPy handle multidimensional arrays?\n",
        "\n",
        " - NumPy excels at handling multidimensional arrays, making it a cornerstone of scientific and numerical computing in Python. These arrays called ndarray can represent everything from simple vectors to complex tensors and image data.\n",
        "\n",
        "     - A 1D array is like a list: [1, 2, 3]\n",
        "     - A 2D array is essentially a matrix with rows and columns.\n",
        "\n",
        "            import numpy as np\n",
        "\n",
        "            # Creating a 2D array\n",
        "            array_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "            print(array_2d)\n",
        "\n",
        "     - A 3D array could represent color images or stacked matrices, often visualized as a cube.\n",
        "\n",
        "            # Creating a 3D array\n",
        "            array_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "            print(array_3d)\n",
        "\n",
        "     - In general, NumPy arrays can have N dimensions.\n",
        "\n",
        " - NumPy's Capabilities :\n",
        "\n",
        "     1. Creation of Multidimensional Arrays.\n",
        "     2. Access and Manipulation\n",
        "       - Use slicing and indexing to access elements.\n",
        "       - Change shapes with .reshape(), .transpose(), or .swapaxes()\n",
        "     3. Vectorized Operations Across Dimensions.\n",
        "     4. Broadcasting\n",
        "       - Automatically stretches shapes to align dimensions.\n",
        "       - Example: Adding a 1D array to each row of a 2D array.\n",
        "\n",
        "16. What is the role of Bokeh in data visualization ?\n",
        "\n",
        " - Bokeh is an open-source Python library for interactive data visualization that primarily targets modern web browsers for rendering. Its role is to enable data scientists and developers to create elegant, concise, and highly interactive plots, dashboards, and data applications that can be easily shared and explored.\n",
        "\n",
        "\n",
        "Here's a breakdown of Bokeh's key roles and strengths in data visualization:\n",
        "\n",
        "  1. Interactive Web-Based Visualizations:\n",
        "\n",
        "    - This is Bokeh's core distinguishing feature. Unlike Matplotlib, which traditionally produces static images, Bokeh generates plots using HTML and JavaScript. This means the visualizations are inherently interactive in a web browser or Jupyter Notebook.\n",
        "    - Users can perform common interactions like zooming, panning, selecting regions, and hovering over data points to reveal tooltips with detailed information, all without writing additional code for these interactions.\n",
        "    - This makes Bokeh ideal for exploratory data analysis where we need to drill down into data and for creating dynamic presentations.\n",
        "\n",
        "  2. Handling Large and Streaming Datasets:\n",
        "\n",
        "    - Bokeh is designed for high-performance visualization of large and even streaming datasets. It achieves this by efficiently sending data to the browser's JavaScript engine, which can often render large numbers of glyphs more effectively than traditional static plotting libraries.\n",
        "    - Its support for streaming data APIs allows us to update plots in real-time, making it suitable for applications like live dashboards, monitoring systems, and financial market visualizations.\n",
        "\n",
        "  3. Building Interactive Dashboards and Applications:\n",
        "\n",
        "    - Beyond just creating individual plots, Bokeh provides tools and layouts to compose multiple plots, widgets, and other UI elements into interactive dashboards and full-fledged web applications.\n",
        "    - The Bokeh server allows we to connect Python code directly to these interactive web visualizations. When a user interacts with a plot or widget in the browser, the Bokeh server can execute Python code in the backend and update the plot dynamically. This enables the creation of sophisticated data exploration tools and custom analytical applications without requiring expertise in front-end web development.\n",
        "\n",
        "  4. Flexibility Across Levels of Abstraction:\n",
        "\n",
        "   -  Bokeh offers different levels of API interfaces to cater to various user needs:\n",
        "       - bokeh.plotting (mid-level): This is the most commonly used interface, similar to Matplotlib's pyplot. It provides functions to quickly create common plots with reasonable defaults and easy customization.We define figures and add \"glyphs\" to them.\n",
        "       - bokeh.models (low-level): This interface gives developers granular control over every aspect of a Bokeh plot, including defining custom plot components, tools, and intricate layouts. It's for users who need maximum flexibility and customization.\n",
        "\n",
        "  5. Integration with the PyData Ecosystem:\n",
        "\n",
        "    - Bokeh integrates well with other popular Python data tools like Pandas and NumPy. We can directly pass Pandas DataFrames or NumPy arrays as data sources for our plots.\n",
        "    - It's also part of the broader HoloViz ecosystem , providing powerful tools for handling and visualizing very large or complex datasets efficiently.\n",
        "\n",
        "17. Explain the difference between apply() and map() in Pandas ?\n",
        "\n",
        " - The apply() and map() methods in Pandas are both used for applying functions to data, but they operate at different levels of granularity and are suitable for different use cases.\n",
        "  1. map()\n",
        "   - Level of Operation: Works element-wise on a Series. We cannot use map() directly on an entire DataFrame.\n",
        "   - Input to Function: Takes a single value as input for each iteration.\n",
        "\n",
        "   - Typical Use Cases:\n",
        "\n",
        "      - Element-wise transformation: Replacing values, converting data types, or applying simple functions to each individual element of a Series.\n",
        "      - Mapping values from a dictionary or Series: This is a very common use case, where we want to replace values in a Series based on a lookup table.\n",
        "\n",
        "  2. apply()\n",
        "   - Level of Operation: Can work on a Series or a DataFrame.\n",
        "\n",
        "   - Input to Function:\n",
        "\n",
        "      - When applied to a Series, it can take a single element or the entire Series as input.\n",
        "      - When applied to a DataFrame, it passes either a Series or a Series to the function, depending on the axis parameter.\n",
        "\n",
        "18. What are some advanced features of NumPy?\n",
        "\n",
        " - NumPy loaded with advanced features that unlock serious computational power for scientists, engineers, and analysts alike.\n",
        "\n",
        "  1. Broadcasting\n",
        "   - Allows operations between arrays of different shapes.\n",
        "   - Reduces need for manual reshaping and looping.\n",
        "           a = np.array([1, 2, 3])\n",
        "           b = 2\n",
        "           print(a + b)  # Broadcasts scalar across array\n",
        "\n",
        "  2. Masked Arrays\n",
        "   - Handle missing or invalid data without corrupting computations.\n",
        "           import numpy.ma as ma\n",
        "           masked = ma.masked_array([1, 2, -99], mask=[False, False, True])\n",
        "           print(masked.mean())  # Ignores masked value\n",
        "\n",
        "  3. Structured Arrays\n",
        "   - Like mini-tables with named fields and mixed data types.\n",
        "           dt = np.dtype([('name', 'U10'), ('age', 'i4')])\n",
        "           data = np.array([('Ashan', 25), ('Bob', 30)], dtype=dt)\n",
        "\n",
        "  4. Memory Mapping\n",
        "   - Work with large datasets without loading everything into RAM.\n",
        "          mmap = np.memmap('data.dat', dtype='float32', mode='r', shape=(1000,))\n",
        "\n",
        "  5. Vectorized UFuncs\n",
        "   - Fast element-wise operations written in C.\n",
        "           a = np.array([1, 2, 3])\n",
        "           print(np.exp(a))  # Apply exponential function element-wise\n",
        "\n",
        "  6. NumPy Broadcasting with np.newaxis\n",
        "   - Reshape arrays on the fly to make broadcasting easier.\n",
        "           a = np.array([1, 2, 3])\n",
        "           print(a[:, np.newaxis])  # Converts 1D to 2D\n",
        "\n",
        "  7. Einstein Summation\n",
        "   - Elegant syntax for multi-dimensional calculations.\n",
        "          a = np.array([[1, 2], [3, 4]])\n",
        "          b = np.array([[5, 6], [7, 8]])\n",
        "          result = np.einsum('ij,jk->ik', a, b)\n",
        "\n",
        "  8. Random Sampling with numpy.random\n",
        "   - Powerful random number generation tools for simulations and ML.\n",
        "          import numpy.random as rnd\n",
        "          rnd.seed(42)\n",
        "          sample = rnd.normal(loc=0, scale=1, size=(3, 3))\n",
        "\n",
        "   9. Multidimensional Array Manipulation\n",
        "   - Advanced reshaping, stacking, splitting, rotating arrays.\n",
        "           a = np.array([[1, 2], [3, 4]])\n",
        "           np.rot90(a)  # Rotate 90 degrees\n",
        "\n",
        "   10. Integration with C/C++/Fortran\n",
        "   - Use NumPy arrays in performance-critical native code through ctypes, cffi, or f2py.\n",
        "\n",
        "\n",
        "19. How does Pandas simplify time series analysis ?\n",
        "  \n",
        "   - Pandas simplifies time series analysis by making it remarkably intuitive to handle dates, times, and chronological data. Whether we're working with financial stock prices, climate patterns, or machine logs, Pandas turns chaotic time-stamped data into structured insights.\n",
        "\n",
        "    1. Date/Time Indexing\n",
        "     - Use DatetimeIndex to treat timestamps as row labels.\n",
        "     - Enables powerful slicing and alignment based on time.\n",
        "             df['date'] = pd.to_datetime(df['date'])\n",
        "             df.set_index('date', inplace=True)\n",
        "    2. Resampling\n",
        "     - Easily convert between different frequencies.\n",
        "     - Use resample() for upsampling, downsampling, and aggregation.\n",
        "             monthly_avg = df.resample('M').mean()\n",
        "\n",
        "    3. Date Ranges and Frequencies\n",
        "     - Create flexible date ranges using date_range().\n",
        "             pd.date_range(start='2022-01-01', end='2022-01-10', freq='D')\n",
        "    \n",
        "    4. Built-in Time Offsets\n",
        "     - Use shortcuts like 'D', 'H', 'M', 'Q', 'Y' to manipulate time with ease.\n",
        "     - Pandas knows how to handle business days, holidays, and custom offsets too.\n",
        "\n",
        "    5. Time Zone Handling\n",
        "      - Support for converting between time zones.\n",
        "               df.index = df.index.tz_localize('UTC').tz_convert('Asia/Kolkata')\n",
        "\n",
        "    6. Rolling Windows\n",
        "     - Perform moving averages, smoothing, and other windowed stats.\n",
        "               df['rolling_mean'] = df['value'].rolling(window=7).mean()\n",
        "\n",
        "    7. Shift & Lag Analysis\n",
        "     - Easily shift data forward or backward for lag features or comparisons.\n",
        "               df['prev_day'] = df['value'].shift(1)\n",
        "\n",
        "\n",
        "20. What is the role of a pivot table in Pandas?\n",
        "\n",
        " - The pivot_table() function in Pandas is an incredibly powerful tool for summarizing and reorganizing data in a DataFrame.It allows us to organize data into a new format based on categories, aggregations, and groupings, making analysis much more insightful and flexibleIts .The primary role is to transform data from a \"long\" or \"tidy\" format into a \"wide\" format, creating a spreadsheet-style summary table that aggregates data based on one or more key columns.\n",
        "\n",
        "    - Summarization: Condense large datasets by aggregating values.\n",
        "    - Restructuring: Reorient data by rows and columns based on field values.\n",
        "    - Comparison: Quickly compare metrics across different categories.\n",
        "    - Filtering: Focus on specific subsets of data in a clean format.\n",
        "\n",
        "  - Example :\n",
        "          import pandas as pd\n",
        "\n",
        "          data = {\n",
        "                'Region': ['North', 'South', 'North', 'South', 'East'],\n",
        "                'Product': ['A', 'A', 'B', 'B', 'A'],\n",
        "                'Sales': [100, 150, 200, 120, 180]\n",
        "                 }\n",
        "\n",
        "          df = pd.DataFrame(data)\n",
        "\n",
        "            pivot = df.pivot_table(values='Sales', index='Region'\n",
        "            columns='Product', aggfunc='sum', fill_value=0)\n",
        "            print(pivot)\n",
        "\n",
        "21. Why is NumPy’s array slicing faster than Python’s list slicing?\n",
        "\n",
        " - NumPy's array slicing is faster than Python's list slicing because it's built for numerical performance.NumPy leverages highly optimized, compiled C code and manages memory in a way that dramatically reduces overhead\n",
        "\n",
        "   1. Contiguous Memory Allocation\n",
        "     - NumPy arrays store data in a contiguous block of memory, unlike Python lists which are arrays of pointers to separate objects.\n",
        "     - This allows for faster access and manipulation because the CPU can prefetch data efficiently.\n",
        "\n",
        "   2. Typed Elements\n",
        "     - All elements in a NumPy array are of the same fixed data type, allowing simpler and faster data interpretation.\n",
        "     - Python lists can hold mixed types, which adds complexity and slows down indexing.\n",
        "\n",
        "   3. No Boxing Overhead\n",
        "     - NumPy avoids “boxing” each number in a Python object wrapper.\n",
        "     - Python lists store each item as a full-fledged Python object, adding overhead on storage and access.\n",
        "\n",
        "   4. Vectorized Internal Operations\n",
        "     - Slicing in NumPy uses views rather than copies.\n",
        "     - That means slicing doesn't duplicate data—it merely creates a new array referencing the original memory.\n",
        "    \n",
        "   5. Optimized C-Level Implementation\n",
        "     - NumPy is implemented in C and Fortran under the hood.\n",
        "     - Slicing operations are performed at a lower level than Python's list mechanics.\n",
        "\n",
        "22. What are some common use cases for Seaborn ?\n",
        "\n",
        " - Seaborn is a high-level Python data visualization library built on top of Matplotlib. Its primary role is to create attractive and informative statistical graphics with less code, making it an indispensable tool for data scientists and analysts.\n",
        "   \n",
        "   1. Exploratory Data Analysis:\n",
        "\n",
        "     - Understanding Distributions: Quickly visualize the distribution of a single numerical variable using histograms, kernel density estimates, or empirical cumulative distribution functions.\n",
        "\n",
        "     - Comparing Distributions: Use box plots, violin plots , or swarm plots to compare the distribution of a numerical variable across different categories.\n",
        "\n",
        "     - Identifying Outliers: Box plots and violin plots are excellent for spotting potential outliers.\n",
        "\n",
        "  2. Visualizing Relationships in Multivariate Data:\n",
        "\n",
        "     - Pair Plots: Generate a grid of scatter plots for every pairwise combination of numerical variables in a dataset, along with univariate distributions on the diagonal. This is a go-to for quick, comprehensive insights into how multiple features interact.\n",
        "\n",
        "     - Joint Plots: Combine a scatter plot for two variables with their marginal distributions on the axes. Useful for deep dives into a specific bivariate relationship.\n",
        "\n",
        "     - Heatmaps: Visualize correlation matrices, covariance matrices, or other matrix-like data. This is excellent for quickly identifying strong positive or negative correlations between features.\n",
        "\n",
        "     - Clustermaps: Perform hierarchical clustering on data and visualize the resulting dendrograms along with a heatmap of the data. Useful for discovering natural groupings in our data.\n",
        "\n",
        "  3. Visualizing Categorical Data :\n",
        "\n",
        "     - Bar plots: Show the mean of a numerical variable for different categories.\n",
        "     - Count plots: Display the number of observations in each category.\n",
        "     - Box plots: Show the distribution of a numerical variable within each category.\n",
        "     - Violin plots: Similar to box plots but also show the probability density of the data at different values.\n",
        "     - Swarm plots: Plot individual observations with non-overlapping points, providing a clearer view of distribution than a simple scatter of points.\n",
        "     - Point plots: Show the mean for each category, often with confidence intervals, highlighting differences across categories.\n",
        "\n",
        "  4. reating Multi-Plot Grids:\n",
        "\n",
        "     - FacetGrid and Figure-level functions: Seaborn allows us to create grids of plots where each subplot shows a subset of the data based on one or more categorical variables. This is incredibly powerful for comparing distributions or relationships across different groups or conditions.\n",
        "     - For example, we can easily create separate scatter plots for male and female customers, or line plots for different product categories, all within a single figure.\n",
        "\n",
        "  5. Enhancing Plot Aesthetics and Themes:\n",
        "\n",
        "     - Seaborn provides beautiful default styles and color palettes that make plots look professional out-of-the-box.\n",
        "     - Functions like sns.set_style(), sns.set_palette(), and sns.despine() allow for easy customization of plot aesthetics without diving into complex Matplotlib syntax.\n",
        "\n",
        "  6. Statistical Estimation and Error Bars:\n",
        "\n",
        "     - Many Seaborn plots automatically calculate and display statistical estimates and their associated uncertainty via error bars, which is crucial for robust statistical analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FVErqPAOBdvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #  Practical\n",
        "\n",
        "# 1.  How do you create a 2D NumPy array and calculate the sum of each row ?\n",
        "\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr_2d = np.array([[1, 2, 3, 4],\n",
        "                   [5, 6, 7, 8],\n",
        "                   [9, 10, 11, 12]])\n",
        "\n",
        "row_sums = np.sum(arr_2d, axis=1)\n",
        "\n",
        "print(\"Original Array:\\n\", arr_2d)\n",
        "print(\"Sum of Each Row:\", row_sums)\n",
        "\n",
        "'''\n",
        "\n",
        "# 2. Write a Pandas script to find the mean of a specific column in a DataFrame.\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Salary': [50000, 60000, 70000],\n",
        "    'Age': [25, 30, 35]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "salary_mean = df['Salary'].mean()\n",
        "\n",
        "print(\"Mean Salary:\", salary_mean)\n",
        "\n",
        "'''\n",
        "\n",
        "# 3. Create a scatter plot using Matplotlib.\n",
        "\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11]\n",
        "y = [99, 86, 87, 88, 100, 86, 103, 87, 94, 78]\n",
        "\n",
        "plt.scatter(x, y, color='purple', marker='o')\n",
        "\n",
        "plt.xlabel(\"X Axis Label\")\n",
        "plt.ylabel(\"Y Axis Label\")\n",
        "plt.title(\"Simple Scatter Plot\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "\n",
        "# 4. How do you calculate the correlation matrix using Seaborn and visualize it with a heatmap.\n",
        "\n",
        "'''\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "data = sns.load_dataset('iris')\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "corr_matrix = data.corr()\n",
        "print(corr_matrix)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Correlation Matrix - Iris Dataset\")\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "\n",
        "# 5. Generate a bar plot using Plotly .\n",
        "\n",
        "'''\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Fruit': ['Apples', 'Bananas', 'Cherries', 'Dates'],\n",
        "    'Quantity': [35, 27, 43, 19]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "fig = px.bar(df, x='Fruit', y='Quantity', title='Fruit Sales', color='Fruit', text='Quantity')\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(xaxis_title='Fruit Type', yaxis_title='Units Sold', showlegend=False)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "'''\n",
        "\n",
        "# 6. Create a DataFrame and add a new column based on an existing column.\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Name': ['Ashan', 'Bob', 'Coco'],\n",
        "    'Salary': [50000, 60000, 70000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Bonus'] = df['Salary'] * 0.10\n",
        "\n",
        "'''\n",
        "\n",
        "# 7.  Write a program to perform element-wise multiplication of two NumPy arrays.\n",
        "\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "array1 = np.array([2, 4, 6, 8])\n",
        "array2 = np.array([1, 3, 5, 7])\n",
        "\n",
        "product = array1 * array2\n",
        "\n",
        "\n",
        "print(\"Array 1:\", array1)\n",
        "print(\"Array 2:\", array2)\n",
        "print(\"Element-wise Multiplication:\", product)\n",
        "\n",
        "'''\n",
        "\n",
        "# 12. Use Pandas to load a CSV file and display its first 5 rows.\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('your_file.csv')  # Replace with your actual file path\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# 13. Create a 3D scatter plot using Plotly.\n",
        "\n",
        "'''\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'X': [1, 2, 3, 4, 5],\n",
        "    'Y': [10, 15, 13, 17, 14],\n",
        "    'Z': [5, 6, 7, 8, 9],\n",
        "    'Label': ['A', 'B', 'C', 'D', 'E']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "fig = px.scatter_3d(df, x='X', y='Y', z='Z', color='Label',\n",
        "                    title='3D Scatter Plot Example')\n",
        "\n",
        "\n",
        "fig.show()\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "SZnCBseJpYXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}